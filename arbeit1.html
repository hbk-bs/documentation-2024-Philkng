<!DOCTYPE html>
<html lang="de">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Kategorie 1: Webentwicklung</title>
  <link rel="stylesheet" href="style.css">
</head>
<body>
  <div class="header-bar">
    <h1>AirGuitar</h1>
  </div>

  <div class="container">
    <h2>AirGuitar – Virtuelle Musik mit Gestensteuerung</h2>
    <p>Das Projekt AirGuitar nutzt die Google Teachable Machine,
       um eine interaktive Website zu schaffen,
        auf der Besucher mit einfachen Handgesten eine Luftgitarre spielen können.
         Die Website erkennt mithilfe von maschinellem Lernen die Bewegungen der Hände und verwandelt sie in musikalische Gitarrentöne,
          die in Echtzeit abgespielt werden.</p>

    <h2>Funktionsweise der AirGuitar</h2>
    <p>
      Durch die Implementierung der Google Teachable Machine werden die Handbewegungen der Nutzer über die Webcam erfasst.
      Die trainierte KI erkennt dabei bestimmte Gesten,
      die dann unterschiedliche Gitarrenklänge auslösen.
      Besucher können die typischen Bewegungen eines Gitarristen nachahmen und ohne physisches Instrument das Gefühl haben,
      Musik zu spielen. Es braucht keine speziellen Geräte – nur eine Webcam und die eigenen Hände.
    </p>

    <!-- Platz für das Bild -->
    <div class="image-placeholder">
      <img src="gitarre.png" alt="Bild von AirGuitar" class="project-image">
    </div>

    <h2>Erkenntnisse im Prozess</h2>
    <p>
      Beim Erstellen dieses Projekts wurde das Verständnis für die Funktionsweise von Teachable Machine vertieft. Es stellte sich heraus, dass eine große Menge an Fotos benötigt wird, um eine effektive Datenbank zu erstellen, obwohl bereits bekannt war, dass Fotos als Grundlage dienen können. Neu war hingegen die Erkenntnis, dass Kategorien im Vorfeld festgelegt werden können, nach denen die KI später selbständig Unterscheidungen treffen kann. Darüber hinaus wurde entdeckt, dass das System auch mit Tönen funktioniert, wobei Bilder mit bestimmten Tönen verknüpft werden können. Der Gedanke hinter dem Arbeiten mit einer KI wurde erst durch dieses Projekt wirklich klar: Es geht darum, die KI mit Daten zu füttern,
       um anschließend zu verstehen, was genau wie erkannt wurde, und gegebenenfalls Anpassungen vorzunehmen.

    <h2>Fazit</h2>
    <p>AirGuitar zeigt, wie moderne Technologien wie maschinelles Lernen und Gestensteuerung genutzt werden können, um interaktive Erlebnisse zu schaffen. Das Projekt bietet einen einfachen Zugang zur Welt der Musik, indem es Bewegungen in Gitarrentöne umwandelt. Allerdings hat es nur teilweise wie geplant funktioniert – der Sound wurde leider unregelmäßig und oft zu schnell hintereinander abgespielt. Es gab jedoch auch positive Aspekte: Besonders gut funktionierte die Möglichkeit, die Tonhöhe mit der Griffhand zu verändern, was den Nutzern ein realistisches Gefühl des Gitarrenspiels vermittelte. Dieser Aspekt war ein gelungenes Element des Projekts, das die kreative Interaktion förderte.
    </p>

    <p> Trotz der technischen Hürden zeigte sich das Potenzial des Projekts. AirGuitar kann mehr sein als eine einfache Spielerei; es bildet die Grundlage für ein breiteres musikalisches Erlebnis. Zukunftspläne beinhalten die Erweiterung um weitere Instrumente wie Schlagzeug oder Bass, um die Performance zu diversifizieren. Zudem könnten Online-Modi integriert werden, die es Nutzern ermöglichen, mit Freunden in Echtzeit zu musizieren. Diese Erweiterungen könnten AirGuitar zu einer vielseitigen Plattform für kreative, virtuelle Musikinteraktionen machen.</p>
            
  
    <a href="https://philsdokumentation.netlify.app/" class="project-link" target="_blank">Besuche die vorgestellte Webseite</a>
   
    <a href="index.html" class="back-link">Zurück zur Startseite</a>
  </div>
</body>
</html>
